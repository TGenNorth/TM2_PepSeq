---
title: "TM2 PepSeq Library Design"
author: "E. Kelley"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

## Analysis Goal:
Design a TM2 PepSeq library for our second cohort of patients for the Tumor Infiltrating Lymphocytes (TIL) Project. 
Exome sequencing and initial sequence analysis was performed at TGen HQ. A collection of files
has been provided by Kevin Drenner in the Sharma lab. Each file corresponds to a unique patient, prefaced by
'C038,' or a unique mouse, 'mm10.' This analysis will gather the data set and perform some pre-processing
steps prior to using an external `oligo_encoding` tool written by Zane Fink in the [Ladner lab at NAU](https://github.com/LadnerLab/Library-Design) to select the best peptides for the final PepSeq library.
  
-----

## R analysis to pre-process peptides  
Start with importing the necessary libraries.  
```{r}
library(tidyverse)
library(MHCbindR)
library(DT)
```

```{r}
create_dt <- function(x) {
  DT::datatable(x,
    extensions = "Buttons",
    options = list(
      dom = "Blfrtip",
      buttons = c("copy", "csv", "excel", "pdf", "print"),
      lengthMenu = list(
        c(10, 25, 50, -1),
        c(10, 25, 50, "All")
      )
    )
  )
}
```

Import tumor mutations data generated by Kevin Drenner at TGen HQ.  
Create a single data frame of all mutations, `data`.
Count the number of rows in each file; this is a rough indicator of the number
of variants in a file. Plot number of rows per file.  
```{r}
files <- dir("data_from_kevin/TILPepseq2_Updated/TILPepseq2_mutations/", pattern = "*.csv")
data <- files %>%
  map_dfr(function(x) {
    read_csv(file.path("data_from_kevin/TILPepseq2_Updated/TILPepseq2_mutations/", x)) %>% mutate(file = x)
  })
glimpse(data)
table(data$file)

p1 <- data %>%
  mutate(file = str_replace(file, ".varCode.csv", replacement = "")) %>%
  ggplot(aes(file)) +
  geom_histogram(stat = "count") +
  coord_flip() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  theme_bw()
p1
```

Select mutations 7, 8, 9 for wildtype and mutant sequences. Gather into a long data frame. Drop
any rows with `NA` for `sequence`. Filter for peptides with `topEffect == TRUE` and 15mers.  
```{r}
peptides_long <- data %>%
  gather(
    key = "peptide", value = "sequence", mutPept7,
    mutPept8, mutPept9, wtPept7, wtPept8, wtPept9
  ) %>%
  select(variantId, effectId, gene_name, topEffect, sourceName, file, peptide, sequence) %>%
  filter(!is.na(sequence)) %>%
  relocate(effectId, .after = sequence)
#create_dt(peptides_long)
```

We only want to include variants with `topEffect` and length=15.  Summarizing
the non-topEffect and non-15mers here to show what has been excluded from the
library design.  
```{r}
not_topEffect <- peptides_long %>%
  filter(topEffect == FALSE)
#create_dt(not_topEffect)

not_15mers <- peptides_long %>%
  filter(!(str_length(sequence) == 15))
#create_dt(not_15mers)
```

After filtering for `topEffect == TRUE`, there are some duplicated sequences remaining.
Some duplicates are across multiple patients, whereas others are across the same patient, just
from another sample/sequence library.  
```{r}
pep_dups <- peptides_long %>%
  filter(topEffect == TRUE) %>%
  ungroup() %>%
  group_by(sequence) %>%
  mutate(dup = n() > 1) %>%
  filter(dup == TRUE)
#create_dt(pep_dups)
```

Need to just include a single peptide sequence for the duplicates in our peptide pool for library design.
`annotated_peptides` won't include information from duplicated sequences, so if we need the metadata for
duplicates, will have to pull from the `pep_dups` data frame.  
```{r}
annotated_peptides <- peptides_long %>%
  filter(topEffect == TRUE) %>%
  filter(str_length(sequence) == 15)
```

Keep only rows with distinct peptide sequences. `distinct` keeps only the first row if there are replicated sequences. `.keep all` retains the other columns.  
```{r}
unique_peptides <- peptides_long %>%
  filter(topEffect == TRUE) %>%
  filter(str_length(sequence) == 15) %>%
  distinct(sequence, .keep_all = TRUE)
```

Create a data frame with named unique peptides to serve as input for the PepSeq Library Design tool; this
data frame is called `named_peptides`. Export `named_peptides.csv` to use as the import for the `oligo_encoding` tool.
```{r}
for_library_design <- unique_peptides %>% select(sequence)
for_library_design <- tibble(for_library_design)
for_library_design$id <- str_c(rep("TM2_", length(for_library_design$sequence)), sprintf("%05d", seq_along(for_library_design$sequence)))

# This file contains 2 columns â€“ the first with one entry for each unique peptide; the second with the peptide identifier (eg "TM1_00001")
named_peptides <- for_library_design %>% select(id, sequence)
#create_dt(named_peptides)
#write_csv(named_peptides, path = "design_outs/named_peptides.csv", col_names = FALSE,  quote = FALSE)
# write_csv(annotated_peptides, path = "design_outs/annotated_peptides.csv")
```
----
## Run `oligo_encoding` script to choose the best peptides for the PepSeq library.  
#### This analysis was performed on our HPC cluster using Conda for package management. Conda yaml file is provided here to allow for reproducibility of the environment. The `oligo_encoding` tool requires a specific version of h2O, along with some other dependencies (details on Ladner lab github). I am using the `oligo_encoding` script from the `Library-Design` tool (pulled from git sha: 206b9299652b5175b11c505139d5b719bda7ce24).

```{bash}
cat analysis/bash/conda_pepseq_design.yml
```

### Step 1 in the `oligo_encoding` analysis.  
#### Generate 10,000 random encodings and select 300 encodings with the lowest deviation from of GC ratio from 0.55 for downstream predictions. This was performed on our HPC cluster using the following ### slurm script; note that you must activate the conda environment prior to running the slurm script.
```{bash}
cat analysis/bash/run_pepseq_design_step1.sh
```

### Step 2 in the `oligo_encoding` analysis. 
#### Select the best oligo encodings using the neural network; `-n 3` will give the three best encodings per peptide. This was performed on our HPC cluster using the following slurm script; note that you must activate the conda environment prior to running the slurm script. This analysis took slightly more than an hour using 2 cores with 40G memory.

```{bash}
cat analysis/bash/run_pepseq_design_step2.sh
```

#### The final `best_encodings` output from `oligo_encoding`:
```{r}
best_encodings <- read_csv("analysis/bash/library_design/best_encodings")
#create_dt

control_orderfile <- read_csv("analysis/R/control_orderfile.csv", col_names = FALSE) %>%
  set_names(c("Seq ID", "Nucleotide Encoding w/ Adapters"))
View(control_orderfile)

encodings_order <- best_encodings %>% select(`Seq ID`, `Nucleotide Encoding w/ Adapters`) %>% arrange(`Seq ID`)

orderfile <- bind_rows(encodings_order, control_orderfile)
#write_csv(orderfile, "20200716_orderfile.csv")

#named_encodings <- best_encodings %>% select(`Seq ID`, `Nucleotide Encoding`) %>% arrange(`Seq ID`)
#order_encodings <- bind_rows(named_encodings, control_orderfile)
```

```{r, echo=FALSE}
#Create a github readme file.
#rmarkdown::render("TM2_PepSeq_preprocess_for_library_design.Rmd", output_format = rmarkdown::github_document(), output_file = "README.md")
```


```{r}
controls_distinct <- control_orderfile %>%
  distinct(`Nucleotide Encoding w/ Adapters`, .keep_all = TRUE)

oligos_distinct <- orderfile %>%
    distinct(`Nucleotide Encoding w/ Adapters`, .keep_all = TRUE)

oligos_duplicated <- orderfile %>%
  group_by(`Nucleotide Encoding w/ Adapters`) %>%
  filter(n()>1)

named_peptides_distinct <- named_peptides %>%
  distinct(sequence, .keep_all = TRUE)

check_best_encodings <- best_encodings %>% 
  group_by(`Nucleotide Encoding w/ Adapters`) %>%
  filter(n()>1)
#write_csv(check_best_encodings, "analysis/R/check_best_encodings.csv")

out_seqs <- read_csv("analysis/R/out_seqs", col_names = FALSE)

count_out_seqs <- out_seqs %>%
  mutate(pep_lib_mem = str_sub(X1, 1, 9)) %>%
  group_by(pep_lib_mem) %>%
  mutate(count = n())

```

Write a .fasta file of the `orderfile`
```{r}
fasta <- character(nrow(orderfile) * 2)
fasta[c(TRUE, FALSE)] <- paste0(">", orderfile$`Seq ID`)
fasta[c(FALSE, TRUE)] <- orderfile$`Nucleotide Encoding w/ Adapters`

#writeLines(fasta, "TM2_coded.fna")
```

Prepare the peptide annotations for PepSeq analysis. 
```{r}
# Import the counts file that John generated.
counts <- read_delim("analysis/demux/counts.tsv", delim = "\t")

# Remove the encoding suffix, eg "_100"
counts_for_sum <- counts %>% 
  mutate(sequence_name = str_replace(`Sequence name`, pattern="_\\d+$", replacement = ""))

# Group by the peptide library member name and sum all counts for the three encodings.
counts_sum <- counts_for_sum %>%
  group_by(sequence_name) %>%
  summarise_if(is.numeric, funs(sum))

# Join the annotation data to the peptide library names.
annotated_peptides_named <- left_join(annotated_peptides, named_peptides)

# Add the PepSeq sequencing counts that John sent.
annotated_peptides_with_counts <- full_join(annotated_peptides_named, counts_sum, by=c("id" = "sequence_name"))

# Double check that the controls in the annotated_peptides_with_counts match the number of controls in the order file, ie did all of the controls make it through the above joins.
count_controls <- control_orderfile %>%
  mutate(sequence_name = str_replace(`Seq ID`, pattern="_\\d+$", replacement = "")) %>%
  pull(sequence_name) %>% unique() %>% length()

# Write the annotated_peptides_with_counts df to a file for downstream analysis by Anna/John
# write_csv(annotated_peptides_with_counts, "analysis/R/annotated_peptides_with_counts.csv")
```

